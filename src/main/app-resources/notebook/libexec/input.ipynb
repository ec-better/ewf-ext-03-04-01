{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##  ewf-ext-03-04-01 - Temperature and Chlorophyll anomalies for coastal areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'ewf-ext-03-04-01 - Temperature and Chlorophyll anomalies for coastal areas'),\n",
    "                ('abstract', 'ewf-ext-03-04-01 - Temperature and Chlorophyll anomalies for coastal areas'),\n",
    "                ('id', 'ewf-ext-03-04-01')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoi = dict([('id', 'yoi'),\n",
    "            ('value', '2019'),\n",
    "            ('title', 'year of interest'),\n",
    "            ('abstract', 'year of interest')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = dict([('id', 'start_year'),\n",
    "            ('value', '2018'),\n",
    "            ('title', 'start year'),\n",
    "            ('abstract', 'start year')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_year = dict([('id', 'end_year'),\n",
    "            ('value', '2019'),\n",
    "            ('title', 'end_year'),\n",
    "            ('abstract', 'end_year')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_of_interest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON ((0.601779726018505 42.8075056025504,5.06836475455413 42.8075056025504,5.06836475455413 39.2520150325718,0.601779726018505 39.2464595785562,0.601779726018505 42.8075056025504))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_of_interest = dict([('id', 'areaOfInterest'),\n",
    "                         ('value', 'europe'),\n",
    "                         ('title', 'Area of the region'),\n",
    "                         ('abstract', 'Area of the region of interest')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_variable = dict([('id', 'imageVariable'),\n",
    "                         ('value', 'sea_surface_temperature'),\n",
    "                         ('title', 'Image Variable'),\n",
    "                         ('abstract', 'Image Variable of Interest')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the Sentinel-1 stack of products' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"        \\ninput_identifiers = ('20170101_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180101_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc',\\n                     '20170201_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180201_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc',\\n                     '20170301_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180301_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc')    \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_identifiers = ('20180101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc', '20190101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc',\n",
    "                      '20180102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc', '20190102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc',\n",
    "                      '20180103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc', '20190103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the Sentinel-1 stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = ( 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20151226T182813_20151226T182838_009217_00D48F_5D5F', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160424T182813_20160424T182838_010967_010769_AA98', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160518T182817_20160518T182842_011317_011291_936E', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160611T182819_20160611T182844_011667_011DC0_391B', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160705T182820_20160705T182845_012017_0128E1_D4EE', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160729T182822_20160729T182847_012367_013456_E8BF', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160822T182823_20160822T182848_012717_013FFE_90AF', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160915T182824_20160915T182849_013067_014B77_1FCD' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/data/CMEMS/SST_MED_SST_L3S_NRT_OBSERVATIONS_010_012_a/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = '/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_output_folder = '/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "import datetime\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# remove contents of a given folder\n",
    "# used to clean a temporary folder\n",
    "def rm_cfolder(folder):\n",
    "    #folder = '/path/to/folder'\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "\n",
    "def crop_image(input_image, polygon_wkt, output_path, product_type=None):\n",
    "    \n",
    "    dataset = None\n",
    "        \n",
    "    if input_image.startswith('ftp://') or input_image.startswith('http'):\n",
    "        try:\n",
    "            dataset = gdal.Open('/vsigzip//vsicurl/%s' % input_image)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif '.nc' in input_image:\n",
    "        dataset = gdal.Open('NETCDF:' + input_image + ':' + product_type)\n",
    "\n",
    "    polygon_ogr = ogr.CreateGeometryFromWkt(polygon_wkt)\n",
    "    envelope = polygon_ogr.GetEnvelope()\n",
    "    bounds = [envelope[0], envelope[3], envelope[1], envelope[2]]         \n",
    "\n",
    "    gdal.Translate(output_path, dataset, outputType=gdal.GDT_Int16, projWin=bounds, projWinSRS='EPSG:4326')\n",
    "\n",
    "    dataset = None\n",
    "\n",
    "\n",
    "    \n",
    "def write_output_image(filepath, output_matrix, image_format, data_format, mask=None, output_projection=None, output_geotransform=None, no_data_value=None):\n",
    "    \n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    \n",
    "    \n",
    "    if mask is not None and mask is not 0:\n",
    "        # TODO: check if output folder exists\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 2, data_format)\n",
    "        mask_band = output.GetRasterBand(2)\n",
    "        mask_band.WriteArray(mask)\n",
    "        if no_data_value is not None:\n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "    else:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    \n",
    "    if filepath is None:\n",
    "        print  \"filepath\"\n",
    "    if output is None:\n",
    "        print  \"output\"\n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326', xRes=output_geotransform[1], yRes=-output_geotransform[5], targetAlignedPixels=True)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "    \n",
    "def calc_max_matrix(mat1, mat2, no_data_value=None):\n",
    "    \n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "    \n",
    "    return np.where(mat1 > mat2, mat1, mat2)\n",
    "\n",
    "\n",
    "def matrix_sum(mat1, mat2, no_data_value=None):\n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "            \n",
    "            \n",
    "    msum = mat1 + mat2\n",
    "    return msum\n",
    "\n",
    "def matrix_sum_for_avg(mat1, mat2, mat_n_vals, no_data_value):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    mat2_0and1s = np.zeros(mat2.shape)\n",
    "    \n",
    "    mat2_0and1s[mat2 != no_data_value] = 1\n",
    "    \n",
    "    \n",
    "    mat_n_vals = mat2_0and1s;\n",
    "    \n",
    "    \n",
    "    #msum = mat1\n",
    "    \n",
    "    msum = np.copy(mat1)\n",
    "    \n",
    "    msum[mat2 != no_data_value] = mat1[mat2 != no_data_value] + mat2[mat2 != no_data_value]\n",
    "    \n",
    "    msum = np.where(np.logical_and(mat1 == no_data_value_alt, mat2 != no_data_value), mat2, msum)\n",
    "    \n",
    "    msum[np.logical_and(mat1 == no_data_value_alt, mat2 == no_data_value) ] = no_data_value\n",
    "\n",
    "    \n",
    "    \n",
    "    #msum = mat1 + mat2\n",
    "\n",
    "    return msum, mat_n_vals\n",
    "\n",
    "\n",
    "def calc_average(matrix_list, n_matrix, no_data_value=None):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    if not isinstance(matrix_list, list):\n",
    "        return 0\n",
    "    \n",
    "    result = np.copy(matrix_list[0])\n",
    "    \n",
    "    result = np.where(result == no_data_value, no_data_value_alt, result)\n",
    "  \n",
    "    mat_n_vals = np.zeros(result.shape)\n",
    "    mat_n_vals[result != no_data_value_alt] = 1\n",
    "    \n",
    "    for i in range(1, len(matrix_list)):\n",
    "     \n",
    "        result, mat_n_vals_of_sum = matrix_sum_for_avg(result, matrix_list[i], mat_n_vals, no_data_value)\n",
    "        \n",
    "        #result = np.copy(result_temp)\n",
    "        \n",
    "        mat_n_vals = mat_n_vals + mat_n_vals_of_sum\n",
    "    \n",
    "\n",
    "    # to avoid division by 0!!\n",
    "    mat_n_vals[mat_n_vals == 0] = no_data_value_alt\n",
    "\n",
    "    result = np.divide(result, mat_n_vals)\n",
    "    \n",
    "    # set as no data value pixels that are no data values in all time series\n",
    "    result[mat_n_vals == no_data_value_alt] = no_data_value\n",
    "    \n",
    "    return result\n",
    "\n",
    "def matrix_sum_for_sigma(mat1, mat2, mat_n_vals, avgmat, no_data_value):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    mat2_0and1s = np.zeros(mat2.shape)\n",
    "    \n",
    "    mat2_0and1s[mat2 != no_data_value] = 1\n",
    "    \n",
    "    \n",
    "    mat_n_vals = mat2_0and1s;\n",
    "    \n",
    "    \n",
    "    #msum = mat1\n",
    "    \n",
    "    msum = np.copy(mat1)\n",
    "    #(matrix_list[0] - avgmat)**2\n",
    "    msum[mat2 != no_data_value] = mat1[mat2 != no_data_value] + (mat2[mat2 != no_data_value] - avgmat[mat2 != no_data_value])**2\n",
    "    \n",
    "    msum = np.where(np.logical_and(mat1 == no_data_value_alt, mat2 != no_data_value), (mat2 - avgmat)**2, msum)\n",
    "    \n",
    "    msum[np.logical_and(mat1 == no_data_value_alt, mat2 == no_data_value) ] = no_data_value\n",
    "\n",
    "    \n",
    "    \n",
    "    #msum = mat1 + mat2\n",
    "\n",
    "    return msum, mat_n_vals\n",
    "\n",
    "\n",
    "def calc_standarddeviation(matrix_list, n_matrix, avgmat, no_data_value=None):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    if not isinstance(matrix_list, list):\n",
    "        return 0\n",
    "    \n",
    "    result = (matrix_list[0] - avgmat)**2\n",
    "    \n",
    "    \n",
    "    result = np.where(matrix_list[0] == no_data_value, no_data_value_alt, result)\n",
    "  \n",
    "    mat_n_vals = np.zeros(result.shape)\n",
    "    mat_n_vals[result != no_data_value_alt] = 1\n",
    "    \n",
    "    for i in range(1, len(matrix_list)):\n",
    "     \n",
    "        result, mat_n_vals_of_sum = matrix_sum_for_sigma(result, matrix_list[i], mat_n_vals, avgmat, no_data_value)\n",
    "        \n",
    "        #result = np.copy(result_temp)\n",
    "        \n",
    "        mat_n_vals = mat_n_vals + mat_n_vals_of_sum\n",
    "    \n",
    "\n",
    "    # to avoid division by 0!!\n",
    "    mat_n_vals[mat_n_vals == 0] = no_data_value_alt\n",
    "\n",
    "    result = np.sqrt(np.divide(result, mat_n_vals))\n",
    "    \n",
    "    # set as no data value pixels that are no data values in all time series\n",
    "    result[mat_n_vals == no_data_value_alt] = no_data_value\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_matrix_list(image_list, product_type):\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    no_data = None\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open('NETCDF:' + img + ':' + product_type)\n",
    "        projection = dataset.GetProjection()\n",
    "        geo_transform = dataset.GetGeoTransform()\n",
    "        no_data = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "    return mat_list, projection, geo_transform, no_data\n",
    "\n",
    "def calculate_anomaly(averagesigma, doy, no_value):\n",
    "    \n",
    "    above = doy > averagesigma\n",
    "    below = doy < averagesigma\n",
    "    \n",
    "    above[averagesigma == no_value] = 0\n",
    "    below[averagesigma == no_value] = 0\n",
    "    \n",
    "    return above*1, below*1\n",
    "    \n",
    "\n",
    "def write_outputs(product_name, first_date, last_date, averages, standard_deviation, image_format, projection, geo_transform, no_data_value):\n",
    "    filenames = []\n",
    "    areaofinterest = area_of_interest['value']\n",
    "    filenames.append(product_name + '_averages_' + areaofinterest + '_' + first_date + '_' + last_date + '.tif')\n",
    "    filenames.append(product_name + '_standarddeviation_' + areaofinterest + '_'+ first_date + '_' + last_date + '.tif')\n",
    "\n",
    "    write_output_image(filenames[0], averages, image_format, gdal.GDT_Int16, None, projection, geo_transform, no_data_value)\n",
    "    write_output_image(filenames[1], standard_deviation, image_format, gdal.GDT_Int16, None, projection, geo_transform, no_data_value)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def isleap(year):\n",
    "    \"\"\"Return True for leap years, False for non-leap years.\"\"\"\n",
    "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    first_date = get_formatted_date(first_date)\n",
    "    last_date = get_formatted_date(last_date)\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))\n",
    "        \n",
    "def get_formatted_date(date_obj):\n",
    "    date = datetime.datetime.strftime(date_obj, '%Y-%m-%dT00:00:00Z')\n",
    "    return date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2019', '2018', '2019')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoi['value'], start_year['value'], end_year['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20180101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20190101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20180102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20190102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20180103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20190103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "file not found on input_identifier => 20180104000000\n",
      "file not found on input_identifier => 20180201000000\n",
      "file not found on input_identifier => 20180301000000\n",
      "file not found on input_identifier => 20180401000000\n",
      "file not found on input_identifier => 20180501000000\n",
      "file not found on input_identifier => 20180601000000\n",
      "file not found on input_identifier => 20180701000000\n",
      "file not found on input_identifier => 20180801000000\n",
      "file not found on input_identifier => 20180901000000\n",
      "file not found on input_identifier => 20181001000000\n",
      "file not found on input_identifier => 20181101000000\n",
      "file not found on input_identifier => 20181201000000\n"
     ]
    }
   ],
   "source": [
    "from calendar import monthrange\n",
    "\n",
    "aoi = region_of_interest['value']\n",
    "yearofinterest = int(yoi['value'])\n",
    "start_year_int = int(start_year['value'])\n",
    "end_year_int = int(end_year['value'])\n",
    "anomaly_year_index = yearofinterest - start_year_int\n",
    "image_variable_value = image_variable['value']\n",
    "areaofinterest = area_of_interest['value']\n",
    "\n",
    "image_variable_value_name = image_variable_value.replace(\"_\", \"\")\n",
    "product_path_name = output_folder + '/' + image_variable_value_name\n",
    "\n",
    "#first_date = os.path.splitext(input_identifiers[0])[0].split('-')[0]\n",
    "#last_date = os.path.splitext(input_identifiers[-1])[0].split('-')[0]\n",
    "\n",
    "#input_identifiers.sort()\n",
    "\n",
    "if input_identifiers[0].find(\"SST\") >=0:     \n",
    "\n",
    "    doy = 0\n",
    "    \n",
    "    product_path_name = output_folder + '/' + image_variable_value_name\n",
    "    below = None\n",
    "    above = None\n",
    "    mask_alltime_novalues = None\n",
    "    average = None\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    no_data = None\n",
    "    for month in range(1,13):\n",
    "        getout = False\n",
    "        for day in range(1, monthrange(yearofinterest, month)[1]+1):\n",
    "            doy+=1\n",
    "            file_list = []\n",
    "            month_and_day = str(month).zfill(2)  + str(day).zfill(2)\n",
    "            for year in range(start_year_int, end_year_int+1):\n",
    "                date = str(year) + month_and_day + \"000000\"\n",
    "                matching = [s for s in input_identifiers if date in s]\n",
    "                if len(matching) == 1:\n",
    "                    file = matching[0]\n",
    "                    file_list.append(file)\n",
    "                else:\n",
    "                    print \"file not found on input_identifier => \" + date\n",
    "                    getout = True\n",
    "                    break\n",
    "            if getout == True:\n",
    "                getout = False\n",
    "                break #continue to try the next day, break next month\n",
    "            first_year = file_list[0].split('-')[0][:4]\n",
    "            last_year = file_list[-1].split('-')[0][:4]\n",
    "            first_date = first_year + str(doy).zfill(3)\n",
    "            last_date = last_year + str(doy).zfill(3)\n",
    "            file_list[:] = [data_path + '/' + filename for filename in file_list]\n",
    "            n_years = end_year_int - start_year_int + 1\n",
    "            \n",
    "            if n_years == len(file_list):\n",
    "                mat_list, projection, geo_transform, no_data = get_matrix_list(file_list, image_variable_value)\n",
    "                average = calc_average(mat_list, len(file_list), no_data)\n",
    "                standarddeviations = calc_standarddeviation(mat_list, len(file_list), average, no_data)\n",
    "                averagesigma = matrix_sum(average, standarddeviations)\n",
    "                #averagesigma[average == no_data] = no_data\n",
    "                above_mat, below_mat = calculate_anomaly(averagesigma, mat_list[anomaly_year_index], no_data)\n",
    "                if (above is None) or (below is None) or (mask_alltime_novalues is None):\n",
    "                    \n",
    "                    above = np.zeros(above_mat.shape)\n",
    "                    below = np.zeros(below_mat.shape)\n",
    "                    \n",
    "                    mask_alltime_novalues = (below == 0)\n",
    "                    \n",
    "                above += above_mat\n",
    "                below += below_mat\n",
    "                \n",
    "                mask_alltime_novalues = np.logical_and(mask_alltime_novalues, (average == no_data))\n",
    "                \n",
    "                for i, file in enumerate(file_list):\n",
    "                    cropped_file = cropped_output_folder + '/'+ file.split('/')[-1]\n",
    "                    print cropped_file\n",
    "                    crop_image(file, aoi, cropped_file, image_variable_value)\n",
    "                    file_list[i] = cropped_file + '.tif'\n",
    "\n",
    "                \n",
    "                files = write_outputs(product_path_name, first_date, last_date, average, standarddeviations, 'GTiff', projection, geo_transform, no_data)\n",
    "                for output_name in files:\n",
    "                    firstdate_obj = datetime.datetime.strptime(first_date, \"%Y%j\").date()\n",
    "                    lastdate_obj = datetime.datetime.strptime(last_date, \"%Y%j\").date()\n",
    "                    write_properties_file(output_name, firstdate_obj, lastdate_obj, region_of_interest)\n",
    "            else:\n",
    "                print \"error\" + str(len(file_list))\n",
    "    #    if getout == True:\n",
    "    #        break\n",
    "else:\n",
    "    product_path_name = output_folder + '/' + image_variable_value_name\n",
    "    below = None\n",
    "    above = None\n",
    "    mask_alltime_novalues = None\n",
    "    average = None\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    no_data = None\n",
    "    for month in range(1,13):\n",
    "        file_list = []\n",
    "        getout = False\n",
    "        for year in range(start_year_int, end_year_int+1):\n",
    "            date = str(year) + str(month).zfill(2) + '01'\n",
    "            matching = [s for s in input_identifiers if date in s]\n",
    "            if len(matching) == 1:\n",
    "                file = matching[0]\n",
    "                file_list.append(file)\n",
    "            else:\n",
    "                print \"file not found on input_identifier => \" + date\n",
    "                getout = True\n",
    "                break\n",
    "        if getout == True:\n",
    "            getout = False\n",
    "            break #continue to try the next day, break next month\n",
    "        first_year = file_list[0].split('-')[0][:4]\n",
    "        last_year = file_list[-1].split('-')[0][:4]\n",
    "        first_date = first_year + str(month).zfill(2)\n",
    "        last_date = last_year + str(month).zfill(2)\n",
    "        file_list[:] = [data_path + '/' + filename for filename in file_list]\n",
    "        n_years = end_year_int - start_year_int + 1\n",
    "        \n",
    "        #meter aqui\n",
    "        \n",
    "        if n_years == len(file_list):\n",
    "            mat_list, projection, geo_transform, no_data = get_matrix_list(file_list, image_variable_value)\n",
    "            \n",
    "                    \n",
    "            average = calc_average(mat_list, len(file_list), no_data)\n",
    "            standarddeviations = calc_standarddeviation(mat_list, len(file_list), average, no_data)\n",
    "            averagesigma = matrix_sum(average, standarddeviations)\n",
    "            #averagesigma[average == no_data] = no_data\n",
    "            above_mat, below_mat = calculate_anomaly(averagesigma, mat_list[anomaly_year_index], no_data)\n",
    "            if (above is None) or (below is None) or (mask_alltime_novalues is None):\n",
    "                \n",
    "                above = np.zeros(above_mat.shape)\n",
    "                below = np.zeros(below_mat.shape)\n",
    "                \n",
    "                mask_alltime_novalues = (below == 0)\n",
    "                \n",
    "            above += above_mat\n",
    "            below += below_mat\n",
    "            \n",
    "            mask_alltime_novalues = np.logical_and(mask_alltime_novalues, (average == no_data))\n",
    "            \n",
    "            for i, file in enumerate(file_list):\n",
    "                    cropped_file = cropped_output_folder + '/'+ file.split('/')[-1]\n",
    "                    print cropped_file\n",
    "                    crop_image(file, aoi, cropped_file, image_variable_value)\n",
    "                    file_list[i] = cropped_file + '.tif'\n",
    "            \n",
    "            files = write_outputs(product_path_name, first_date, last_date, average, standarddeviations, 'GTiff', projection, geo_transform, no_data)\n",
    "            for output_name in files:\n",
    "                firstdate_obj = datetime.datetime.strptime(first_date, \"%Y%j\").date()\n",
    "                lastdate_obj = datetime.datetime.strptime(last_date, \"%Y%j\").date()\n",
    "                write_properties_file(output_name, first_date, last_date, region_of_interest)\n",
    "        else:\n",
    "            print \"error\" + str(len(file_list))\n",
    "    #    if getout == True:\n",
    "    #        break\n",
    "\n",
    "\n",
    "above[mask_alltime_novalues] = no_data\n",
    "below[mask_alltime_novalues] = no_data\n",
    "\n",
    "file = write_output_image(product_path_name + '_above_' + areaofinterest + '_' + first_year + '_' + last_year + '.tif', above, 'GTiff', gdal.GDT_Int16, None, projection, geo_transform, no_data)\n",
    "firstdate_obj = datetime.datetime.strptime(first_year, \"%Y\").date()\n",
    "lastdate_obj = datetime.datetime.strptime(last_year, \"%Y\").date()\n",
    "write_properties_file(file, firstdate_obj, lastdate_obj, region_of_interest)\n",
    "write_output_image(product_path_name + '_below_' + areaofinterest + '_' + first_year + '_' + last_year + '.tif', below, 'GTiff', gdal.GDT_Int16, None, projection, geo_transform, no_data)\n",
    "write_properties_file(output_name, firstdate_obj, lastdate_obj, region_of_interest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##  ewf-ext-03-04-01 - Temperature and Chlorophyll anomalies for coastal areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'ewf-ext-03-04-01 - Temperature and Chlorophyll anomalies for coastal areas'),\n",
    "                ('id', 'ewf-ext-03-04-01')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoi = dict([('id', 'yoi'),\n",
    "            ('value', '2019'),\n",
    "            ('title', 'year of interest'),\n",
    "            ('abstract', 'year of interest')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = dict([('id', 'start_year'),\n",
    "            ('value', '2018'),\n",
    "            ('title', 'start year'),\n",
    "            ('abstract', 'start year')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_year = dict([('id', 'end_year'),\n",
    "            ('value', '2019'),\n",
    "            ('title', 'end_year'),\n",
    "            ('abstract', 'end_year')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_of_interest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON ((0.601779726018505 42.8075056025504,5.06836475455413 42.8075056025504,5.06836475455413 39.2520150325718,0.601779726018505 39.2464595785562,0.601779726018505 42.8075056025504))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_of_interest = dict([('id', 'areaOfInterest'),\n",
    "                         ('value', 'europe'),\n",
    "                         ('title', 'Area of the region'),\n",
    "                         ('abstract', 'Area of the region of interest')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_variable = dict([('id', 'imageVariable'),\n",
    "                         ('value', 'sea_surface_temperature'),\n",
    "                         ('title', 'Image Variable'),\n",
    "                         ('abstract', 'Image Variable of Interest')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the Sentinel-1 stack of products' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"        \\ninput_identifiers = ('20170101_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180101_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc',\\n                     '20170201_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180201_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc',\\n                     '20170301_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180301_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc')    \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_identifiers = ('20180101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc', '20190101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc',\n",
    "                      '20180102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc', '20190102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc',\n",
    "                      '20180103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc', '20190103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc')\n",
    "'''        \n",
    "input_identifiers = ('20170101_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180101_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc',\n",
    "                     '20170201_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180201_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc',\n",
    "                     '20170301_m-OGS--BIOL-MedBFM1-MED-b20190402_re-sv04.10.nc','20180301_m-OGS--BIOL-MedBFM1-MED-b20191101_re-sv04.10.nc')    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the Sentinel-1 stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#input_references = ( 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20151226T182813_20151226T182838_009217_00D48F_5D5F', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160424T182813_20160424T182838_010967_010769_AA98', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160518T182817_20160518T182842_011317_011291_936E', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160611T182819_20160611T182844_011667_011DC0_391B', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160705T182820_20160705T182845_012017_0128E1_D4EE', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160729T182822_20160729T182847_012367_013456_E8BF', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160822T182823_20160822T182848_012717_013FFE_90AF', 'https://catalog.terradue.com/sentinel1/search?uid=S1A_IW_GRDH_1SDV_20160915T182824_20160915T182849_013067_014B77_1FCD' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/data/CMEMS/SST_MED_SST_L3S_NRT_OBSERVATIONS_010_012_a/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = '/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_output_folder = '/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# remove contents of a given folder\n",
    "# used to clean a temporary folder\n",
    "def rm_cfolder(folder):\n",
    "    #folder = '/path/to/folder'\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "'''\n",
    "def crop_image(input_image, polygon_wkt, output_path, product_type=None):    \n",
    "    \n",
    "    dataset = None\n",
    "    \n",
    "    # creates directory if it doesnt exist yet\n",
    "    crop_directory = os.path.dirname(output_path)\n",
    "    if crop_directory is not '' and not os.path.exists(crop_directory):\n",
    "        os.makedirs(crop_directory)\n",
    "        \n",
    "    \n",
    "    if input_image.startswith('ftp://') or input_image.startswith('http'):\n",
    "        try:\n",
    "            dataset = gdal.Open('/vsigzip//vsicurl/%s' % input_image)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif '.nc' in input_image:\n",
    "        dataset = gdal.Open('NETCDF:' + input_image + ':' + product_type)\n",
    "        \n",
    "    else: # .tif\n",
    "        dataset = gdal.Open(input_image)\n",
    "        \n",
    "    \n",
    "    no_data_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "    geo_t = dataset.GetGeoTransform()\n",
    "    polygon_ogr = ogr.CreateGeometryFromWkt(polygon_wkt)\n",
    "    envelope = polygon_ogr.GetEnvelope()\n",
    "    bounds = [envelope[0], envelope[2], envelope[1], envelope[3]]\n",
    "    gdal.Warp(output_path, dataset, format=\"GTiff\", outputBoundsSRS='EPSG:4326', outputBounds=bounds, srcNodata=no_data_value, dstNodata=no_data_value, xRes=geo_t[1], yRes=-geo_t[5], targetAlignedPixels=True)\n",
    "'''   \n",
    "\n",
    "def crop_image(input_image, polygon_wkt, output_path, product_type=None):\n",
    "    \n",
    "    dataset = None\n",
    "        \n",
    "    if input_image.startswith('ftp://') or input_image.startswith('http'):\n",
    "        try:\n",
    "            dataset = gdal.Open('/vsigzip//vsicurl/%s' % input_image)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif '.nc' in input_image:\n",
    "        dataset = gdal.Open('NETCDF:' + input_image + ':' + product_type)\n",
    "\n",
    "    polygon_ogr = ogr.CreateGeometryFromWkt(polygon_wkt)\n",
    "    envelope = polygon_ogr.GetEnvelope()\n",
    "    bounds = [envelope[0], envelope[3], envelope[1], envelope[2]]         \n",
    "\n",
    "    gdal.Translate(output_path, dataset, outputType=gdal.GDT_Int16, projWin=bounds, projWinSRS='EPSG:4326')\n",
    "\n",
    "    dataset = None\n",
    "\n",
    "\n",
    "    \n",
    "def write_output_image(filepath, output_matrix, image_format, data_format, mask=None, output_projection=None, output_geotransform=None, no_data_value=None):\n",
    "    \n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    \n",
    "    \n",
    "    if mask is not None and mask is not 0:\n",
    "        # TODO: check if output folder exists\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 2, data_format)\n",
    "        mask_band = output.GetRasterBand(2)\n",
    "        mask_band.WriteArray(mask)\n",
    "        if no_data_value is not None:\n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "    else:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    \n",
    "    if filepath is None:\n",
    "        print  \"filepath\"\n",
    "    if output is None:\n",
    "        print  \"output\"\n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326', xRes=output_geotransform[1], yRes=-output_geotransform[5], targetAlignedPixels=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def calc_max_matrix(mat1, mat2, no_data_value=None):\n",
    "    \n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "    \n",
    "    return np.where(mat1 > mat2, mat1, mat2)\n",
    "\n",
    "\n",
    "def matrix_sum(mat1, mat2, no_data_value=None):\n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "            \n",
    "            \n",
    "    msum = mat1 + mat2\n",
    "    return msum\n",
    "            \n",
    "'''\n",
    "def calc_average(matrix_list, n_matrix, no_data_value=None):\n",
    "    if not matrix_list:\n",
    "        return 0\n",
    "    result = matrix_list[0]\n",
    "    for i in range(1, n_matrix):\n",
    "        result = matrix_sum(result, matrix_list[i], no_data_value)\n",
    "    \n",
    "    return np.divide(result, (n_matrix*1.00))\n",
    "'''\n",
    "def matrix_sum_for_avg(mat1, mat2, mat_n_vals, no_data_value):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    mat2_0and1s = np.zeros(mat2.shape)\n",
    "    \n",
    "    mat2_0and1s[mat2 != no_data_value] = 1\n",
    "    \n",
    "    \n",
    "    mat_n_vals = mat2_0and1s;\n",
    "    \n",
    "    \n",
    "    #msum = mat1\n",
    "    \n",
    "    msum = np.copy(mat1)\n",
    "    \n",
    "    msum[mat2 != no_data_value] = mat1[mat2 != no_data_value] + mat2[mat2 != no_data_value]\n",
    "    \n",
    "    msum = np.where(np.logical_and(mat1 == no_data_value_alt, mat2 != no_data_value), mat2, msum)\n",
    "    \n",
    "    msum[np.logical_and(mat1 == no_data_value_alt, mat2 == no_data_value) ] = no_data_value\n",
    "\n",
    "    \n",
    "    \n",
    "    #msum = mat1 + mat2\n",
    "\n",
    "    return msum, mat_n_vals\n",
    "\n",
    "\n",
    "def calc_average(matrix_list, n_matrix, no_data_value=None):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    if not isinstance(matrix_list, list):\n",
    "        return 0\n",
    "    \n",
    "    result = np.copy(matrix_list[0])\n",
    "    \n",
    "    result = np.where(result == no_data_value, no_data_value_alt, result)\n",
    "  \n",
    "    mat_n_vals = np.zeros(result.shape)\n",
    "    mat_n_vals[result != no_data_value_alt] = 1\n",
    "    \n",
    "    for i in range(1, len(matrix_list)):\n",
    "     \n",
    "        result, mat_n_vals_of_sum = matrix_sum_for_avg(result, matrix_list[i], mat_n_vals, no_data_value)\n",
    "        \n",
    "        #result = np.copy(result_temp)\n",
    "        \n",
    "        mat_n_vals = mat_n_vals + mat_n_vals_of_sum\n",
    "    \n",
    "\n",
    "    # to avoid division by 0!!\n",
    "    mat_n_vals[mat_n_vals == 0] = no_data_value_alt\n",
    "\n",
    "    result = np.divide(result, mat_n_vals)\n",
    "    \n",
    "    # set as no data value pixels that are no data values in all time series\n",
    "    result[mat_n_vals == no_data_value_alt] = no_data_value\n",
    "    \n",
    "    return result\n",
    "\n",
    "'''\n",
    "def calc_standarddeviation(matrix_list, average_matrix, no_data_value=None):\n",
    "    if not matrix_list:\n",
    "        return 0\n",
    "    result = np.zeros(average_matrix.shape)\n",
    "    for matrix in matrix_list: \n",
    "        result = matrix_sum(np.square(matrix_sum(matrix, np.negative(average_matrix))), result)              \n",
    "    return np.sqrt(np.divide(result, (len(matrix_list)*1.00)))\n",
    "'''\n",
    "\n",
    "def matrix_sum_for_sigma(mat1, mat2, mat_n_vals, avgmat, no_data_value):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    mat2_0and1s = np.zeros(mat2.shape)\n",
    "    \n",
    "    mat2_0and1s[mat2 != no_data_value] = 1\n",
    "    \n",
    "    \n",
    "    mat_n_vals = mat2_0and1s;\n",
    "    \n",
    "    \n",
    "    #msum = mat1\n",
    "    \n",
    "    msum = np.copy(mat1)\n",
    "    #(matrix_list[0] - avgmat)**2\n",
    "    msum[mat2 != no_data_value] = mat1[mat2 != no_data_value] + (mat2[mat2 != no_data_value] - avgmat[mat2 != no_data_value])**2\n",
    "    \n",
    "    msum = np.where(np.logical_and(mat1 == no_data_value_alt, mat2 != no_data_value), (mat2 - avgmat)**2, msum)\n",
    "    \n",
    "    msum[np.logical_and(mat1 == no_data_value_alt, mat2 == no_data_value) ] = no_data_value\n",
    "\n",
    "    \n",
    "    \n",
    "    #msum = mat1 + mat2\n",
    "\n",
    "    return msum, mat_n_vals\n",
    "\n",
    "\n",
    "def calc_standarddeviation(matrix_list, n_matrix, avgmat, no_data_value=None):\n",
    "\n",
    "    no_data_value_alt = -32768\n",
    "    \n",
    "    if not isinstance(matrix_list, list):\n",
    "        return 0\n",
    "    \n",
    "    result = (matrix_list[0] - avgmat)**2\n",
    "    \n",
    "    \n",
    "    result = np.where(matrix_list[0] == no_data_value, no_data_value_alt, result)\n",
    "  \n",
    "    mat_n_vals = np.zeros(result.shape)\n",
    "    mat_n_vals[result != no_data_value_alt] = 1\n",
    "    \n",
    "    for i in range(1, len(matrix_list)):\n",
    "     \n",
    "        result, mat_n_vals_of_sum = matrix_sum_for_sigma(result, matrix_list[i], mat_n_vals, avgmat, no_data_value)\n",
    "        \n",
    "        #result = np.copy(result_temp)\n",
    "        \n",
    "        mat_n_vals = mat_n_vals + mat_n_vals_of_sum\n",
    "    \n",
    "\n",
    "    # to avoid division by 0!!\n",
    "    mat_n_vals[mat_n_vals == 0] = no_data_value_alt\n",
    "\n",
    "    result = np.sqrt(np.divide(result, mat_n_vals))\n",
    "    \n",
    "    # set as no data value pixels that are no data values in all time series\n",
    "    result[mat_n_vals == no_data_value_alt] = no_data_value\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_matrix_list(image_list, product_type):\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    no_data = None\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open('NETCDF:' + img + ':' + product_type)\n",
    "        projection = dataset.GetProjection()\n",
    "        geo_transform = dataset.GetGeoTransform()\n",
    "        no_data = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "    return mat_list, projection, geo_transform, no_data\n",
    "\n",
    "def calculate_anomaly(averagesigma, doy):\n",
    "    above = averagesigma > doy\n",
    "    below = averagesigma < doy\n",
    "    return above*1, below*1\n",
    "    \n",
    "\n",
    "def write_outputs(product_name, first_date, last_date, averages, standard_deviation, image_format, projection, geo_transform, no_data_value):\n",
    "    filenames = []\n",
    "    areaofinterest = area_of_interest['value']\n",
    "    filenames.append(product_name + '_averages_' + areaofinterest + '_' + first_date + '_' + last_date + '.tif')\n",
    "    filenames.append(product_name + '_standarddeviation_' + areaofinterest + '_'+ first_date + '_' + last_date + '.tif')\n",
    "\n",
    "    write_output_image(filenames[0], averages, image_format, gdal.GDT_Int16, None, projection, geo_transform, no_data_value)\n",
    "    write_output_image(filenames[1], standard_deviation, image_format, gdal.GDT_Int16, None, projection, geo_transform, no_data_value)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def isleap(year):\n",
    "    \"\"\"Return True for leap years, False for non-leap years.\"\"\"\n",
    "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2019', '2018', '2019')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoi['value'], start_year['value'], end_year['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20180101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20190101000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20180102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20190102000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20180103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "/workspace/Better_3rd_phase/Applications/EXT-03-04-01/ewf-ext-03-04-01/src/main/app-resources/notebook/libexec/Output/Crop/20190103000000-GOS-L3S_GHRSST-SSTsubskin-night_SST_HR_NRT-MED-v02.0-fv01.0.nc\n",
      "file not found on input_identifier => 20180104000000\n",
      "file not found on input_identifier => 20180201000000\n",
      "file not found on input_identifier => 20180301000000\n",
      "file not found on input_identifier => 20180401000000\n",
      "file not found on input_identifier => 20180501000000\n",
      "file not found on input_identifier => 20180601000000\n",
      "file not found on input_identifier => 20180701000000\n",
      "file not found on input_identifier => 20180801000000\n",
      "file not found on input_identifier => 20180901000000\n",
      "file not found on input_identifier => 20181001000000\n",
      "file not found on input_identifier => 20181101000000\n",
      "file not found on input_identifier => 20181201000000\n"
     ]
    }
   ],
   "source": [
    "from calendar import monthrange\n",
    "\n",
    "aoi = region_of_interest['value']\n",
    "yearofinterest = int(yoi['value'])\n",
    "start_year_int = int(start_year['value'])\n",
    "end_year_int = int(end_year['value'])\n",
    "anomaly_year_index = yearofinterest - start_year_int\n",
    "image_variable_value = image_variable['value']\n",
    "areaofinterest = area_of_interest['value']\n",
    "\n",
    "image_variable_value_name = image_variable_value.replace(\"_\", \"\")\n",
    "product_path_name = output_folder + '/' + image_variable_value_name\n",
    "\n",
    "#first_date = os.path.splitext(input_identifiers[0])[0].split('-')[0]\n",
    "#last_date = os.path.splitext(input_identifiers[-1])[0].split('-')[0]\n",
    "\n",
    "#input_identifiers.sort()\n",
    "\n",
    "if input_identifiers[0].find(\"SST\") >=0:     \n",
    "\n",
    "    doy = 0\n",
    "    \n",
    "    product_path_name = output_folder + '/' + image_variable_value_name\n",
    "    below = None\n",
    "    above = None\n",
    "    average = None\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    no_data = None\n",
    "    for month in range(1,13):\n",
    "        getout = False\n",
    "        for day in range(1, monthrange(yearofinterest, month)[1]+1):\n",
    "            doy+=1\n",
    "            file_list = []\n",
    "            month_and_day = str(month).zfill(2)  + str(day).zfill(2)\n",
    "            for year in range(start_year_int, end_year_int+1):\n",
    "                date = str(year) + month_and_day + \"000000\"\n",
    "                matching = [s for s in input_identifiers if date in s]\n",
    "                if len(matching) == 1:\n",
    "                    file = matching[0]\n",
    "                    file_list.append(file)\n",
    "                else:\n",
    "                    print \"file not found on input_identifier => \" + date\n",
    "                    getout = True\n",
    "                    break\n",
    "            if getout == True:\n",
    "                getout = False\n",
    "                break #continue to try the next day, break next month\n",
    "            first_year = file_list[0].split('-')[0][:4]\n",
    "            last_year = file_list[-1].split('-')[0][:4]\n",
    "            first_date = first_year + str(doy).zfill(3)\n",
    "            last_date = last_year + str(doy).zfill(3)\n",
    "            file_list[:] = [data_path + '/' + filename for filename in file_list]\n",
    "            n_years = end_year_int - start_year_int + 1\n",
    "            \n",
    "            if n_years == len(file_list):\n",
    "                mat_list, projection, geo_transform, no_data = get_matrix_list(file_list, image_variable_value)\n",
    "                average = calc_average(mat_list, len(file_list), -32768)\n",
    "                standarddeviations = calc_standarddeviation(mat_list, len(file_list), average, -32768)\n",
    "                averagesigma = matrix_sum(average, standarddeviations)\n",
    "                above_mat, below_mat = calculate_anomaly(averagesigma, mat_list[anomaly_year_index])\n",
    "                if (above is None) or (below is None):\n",
    "                    above = np.zeros(above_mat.shape)\n",
    "                    below = np.zeros(below_mat.shape)\n",
    "                above += above_mat\n",
    "                below += below_mat\n",
    "                \n",
    "                for i, file in enumerate(file_list):\n",
    "                    cropped_file = cropped_output_folder + '/'+ file.split('/')[-1]\n",
    "                    print cropped_file\n",
    "                    crop_image(file, aoi, cropped_file, image_variable_value)\n",
    "                    file_list[i] = cropped_file + '.tif'\n",
    "\n",
    "                write_outputs(product_path_name, first_date, last_date, average, standarddeviations, 'GTiff', projection, geo_transform, no_data)\n",
    "            else:\n",
    "                print \"error\" + str(len(file_list))\n",
    "    #    if getout == True:\n",
    "    #        break\n",
    "else:\n",
    "    product_path_name = output_folder + '/' + image_variable_value_name\n",
    "    below = None\n",
    "    above = None\n",
    "    average = None\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    no_data = None\n",
    "    for month in range(1,13):\n",
    "        file_list = []\n",
    "        getout = False\n",
    "        for year in range(start_year_int, end_year_int+1):\n",
    "            date = str(year) + str(month).zfill(2) + '01'\n",
    "            matching = [s for s in input_identifiers if date in s]\n",
    "            if len(matching) == 1:\n",
    "                file = matching[0]\n",
    "                file_list.append(file)\n",
    "            else:\n",
    "                print \"file not found on input_identifier => \" + date\n",
    "                getout = True\n",
    "                break\n",
    "        if getout == True:\n",
    "            getout = False\n",
    "            break #continue to try the next day, break next month\n",
    "        first_year = file_list[0].split('-')[0][:4]\n",
    "        last_year = file_list[-1].split('-')[0][:4]\n",
    "        first_date = first_year + str(month).zfill(2)\n",
    "        last_date = last_year + str(month).zfill(2)\n",
    "        file_list[:] = [data_path + '/' + filename for filename in file_list]\n",
    "        n_years = end_year_int - start_year_int + 1\n",
    "        \n",
    "        #meter aqui\n",
    "        \n",
    "        if n_years == len(file_list):\n",
    "            mat_list, projection, geo_transform, no_data = get_matrix_list(file_list, image_variable_value)\n",
    "            \n",
    "                    \n",
    "            average = calc_average(mat_list, len(file_list), -32768)\n",
    "            standarddeviations = calc_standarddeviation(mat_list, len(file_list), average, -32768)\n",
    "            averagesigma = matrix_sum(average, standarddeviations)\n",
    "            above_mat, below_mat = calculate_anomaly(averagesigma, mat_list[anomaly_year_index])\n",
    "            if (above is None) or (below is None):\n",
    "                above = np.zeros(above_mat.shape)\n",
    "                below = np.zeros(below_mat.shape)\n",
    "            above += above_mat\n",
    "            below += below_mat\n",
    "            \n",
    "            for i, file in enumerate(file_list):\n",
    "                    cropped_file = cropped_output_folder + '/'+ file.split('/')[-1]\n",
    "                    print cropped_file\n",
    "                    crop_image(file, aoi, cropped_file, image_variable_value)\n",
    "                    file_list[i] = cropped_file + '.tif'\n",
    "            \n",
    "            write_outputs(product_path_name, first_date, last_date, average, standarddeviations, 'GTiff', projection, geo_transform, no_data)\n",
    "        else:\n",
    "            print \"error\" + str(len(file_list))\n",
    "    #    if getout == True:\n",
    "    #        break\n",
    "\n",
    "\n",
    "write_output_image(product_path_name + '_above_' + areaofinterest + '_' + first_year + '_' + last_year + '.tif', above, 'GTiff', gdal.GDT_Int16, None, projection, geo_transform, no_data)\n",
    "write_output_image(product_path_name + '_below_' + areaofinterest + '_' + first_year + '_' + last_year + '.tif', below, 'GTiff', gdal.GDT_Int16, None, projection, geo_transform, no_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data can not convert to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e4746d5857b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3158\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5122\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5124\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5125\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/matplotlib/image.pyc\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    594\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    595\u001b[0m                 not np.can_cast(self._A.dtype, np.float)):\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image data can not convert to float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         if (self._A.ndim not in (2, 3) or\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data can not convert to float"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(average)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(standarddeviations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(averagesigma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_variable['value'] = image_variable['value'].replace(\"_\", \"\")\n",
    "\n",
    "product_path_name = output_folder + '/' + image_variable\n",
    "\n",
    "filenames = write_outputs(product_path_name, first_date, last_date, averages, standarddeviations, 'GTiff', projection, geo_transform, no_data)\n",
    "\n",
    "# TODO\n",
    "#write_properties_file(interval_gpd, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "\n",
    "import rasterio\n",
    "\n",
    "from rasterio.plot import show\n",
    "\n",
    "\n",
    "# Load data\n",
    "#raster = output_folder + '/' + 'LST_SouthernAfrica_N3_maxvalues_'+first_date+'_'+last_date+'.tif'\n",
    "raster = filenames[0]\n",
    "data = rasterio.open(raster)\n",
    "\n",
    "show(data)\n",
    "\n",
    "#data = georasters.from_file(raster)\n",
    "\n",
    "# Plot data\n",
    "#data.plot()\n",
    "\n",
    "#print(no_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
